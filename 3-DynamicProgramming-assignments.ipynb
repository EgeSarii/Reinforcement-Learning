{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reinforcement Learning 3: *Dynamic Programming*\n",
    "\n",
    "**Assignment:** hand-in before 21/02/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **1. Frozen Lake**\n",
    "\n",
    "*(5 x 1 points)*\n",
    "\n",
    "The Frozen Lake environment is a 4×4 grid which contain four possible areas  — Safe (S), Frozen (F), Hole (H) and Goal (G). The agent moves around the grid until it reaches the goal or the hole. The agent in the environment has four possible moves — Up, Down, Left and Right. If it falls into the hole, it has to start from the beginning and is rewarded the value 0.\n",
    "The process continues until it learns from every mistake and reaches the goal. Here is a visual description of the Frozen Lake grid task:\n",
    "\n",
    "![](./data/FrozenLake.png)\n",
    "\n",
    "Note that the ice is slippery, so the agent won't always move in the direction intended by the action. Specifically, there is a 1/3 chance of moving in the direction prescribed by the action and 1/3 to each orthogonal direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if you don't have gym installed, uncomment and run these lines\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "print(envs.registry.all()) # check if 'FrozenLake-v1' and 'FrozenLake8x8-v1' are among the environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the code provided below, an agent taking random actions is interacting with this environment (i.e., the initial policy is a deterministic random policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_episodes(environment, n_episodes, policy, display=True):\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    for episode in range(n_episodes):\n",
    "        terminated = False\n",
    "        state = environment.reset()\n",
    "        while not terminated:\n",
    "            # Select an action to perform in a current state\n",
    "            if policy == 'random':\n",
    "                action = environment.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(policy[state])\n",
    "\n",
    "            # Perform an action and observe how environment acted in response\n",
    "            next_state, reward, terminated, info = environment.step(action)\n",
    "\n",
    "            # Plot the first episode\n",
    "            if episode==1 and display:\n",
    "                print(\"Action:\")\n",
    "                environment.render() # display current agent state\n",
    "            # Summarize total reward\n",
    "            total_reward += reward\n",
    "            # Update current state\n",
    "            state = next_state\n",
    "            # Calculate number of wins over episodes\n",
    "            if terminated and reward == 1.0:\n",
    "                wins += 1\n",
    "    average_reward = total_reward / n_episodes\n",
    "    return wins, total_reward, average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load a Frozen Lake environment\n",
    "env = gym.make('FrozenLake-v1')\n",
    "# Number of episodes to play\n",
    "n_episodes = 5000\n",
    "# First episode plotted as a sample episode\n",
    "print('First episode:')\n",
    "wins, total_reward, average_reward = run_episodes(env, n_episodes, policy=\"random\")\n",
    "print('------------------------------------')\n",
    "print('Summary:')\n",
    "print(f'- number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'- average reward over {n_episodes} episodes = {average_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**a**) Implement the Iterative Policy Evaluation algorithm as a function to evaluate the given policy. How many iterations does the random policy need to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # Number of evaluation iterations\n",
    "    evaluation_iterations = 1\n",
    "    # Initialize a value function for each state as zero\n",
    "    V = np.zeros(environment.nS)\n",
    "    # Repeat until change in value is below the threshold\n",
    "    for i in range(int(max_iterations)):\n",
    "        # Initialize a change of value function as zero\n",
    "        delta = 0\n",
    "        # Iterate though each state\n",
    "        for state in range(environment.nS):\n",
    "            # TODO your code here\n",
    "            # v =\n",
    "\n",
    "            # Calculate the absolute change of value function\n",
    "            delta = max(delta, np.abs(V[state] - v))\n",
    "            # Update value function\n",
    "            V[state] = v\n",
    "        evaluation_iterations += 1\n",
    "\n",
    "        # Terminate if value change is insignificant\n",
    "        if delta < theta:\n",
    "            # TODO - check how many iterations\n",
    "            return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**b**) Using your Policy Evaluation function from (a), implement the Policy iteration algorithm. Run the Policy iteration to obtain the optimal policy for the `FrozenLake-v1` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def one_step_lookahead(environment, state, V, discount_factor):\n",
    "    action_values = np.zeros(environment.nA)\n",
    "    for action in range(environment.nA):\n",
    "        for probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "            action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
    "    # Start with a random policy\n",
    "    # num states x num actions / num actions\n",
    "    policy = np.ones([environment.nS, environment.nA]) / environment.nA\n",
    "    # Initialize counter of evaluated policies\n",
    "    evaluated_policies = 1\n",
    "    # Repeat until convergence or critical number of iterations reached\n",
    "    for i in range(int(max_iterations)):\n",
    "        stable_policy = True\n",
    "        # Evaluate current policy\n",
    "        V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
    "        # Go through each state and try to improve actions that were taken (policy Improvement)\n",
    "        for state in range(environment.nS):\n",
    "            # Choose the best action in a current state under current policy\n",
    "            # current_action =\n",
    "            # Look one step ahead and evaluate if current action is optimal\n",
    "            # We will try every possible action in a current state\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # Select a better action\n",
    "            # best_action =\n",
    "            # If action didn't change\n",
    "            if current_action != best_action:\n",
    "                stable_policy = True\n",
    "                # Greedy policy update\n",
    "                policy[state] = np.eye(environment.nA)[best_action]\n",
    "        evaluated_policies += 1\n",
    "        # If the algorithm converged and policy is not changing anymore, then return final policy and value function\n",
    "        if stable_policy:\n",
    "            return policy, V"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 1000\n",
    "iteration_name = \"Policy iteration\"\n",
    "iteration_func = policy_iteration\n",
    "# Load a Frozen Lake environment\n",
    "environment = gym.make('FrozenLake-v1')\n",
    "# Search for an optimal policy using policy iteration\n",
    "policy, V = iteration_func(environment.env)\n",
    "# Apply best policy to the real environment\n",
    "wins, total_reward, average_reward = run_episodes(environment, n_episodes, policy)\n",
    "print(f'{iteration_name}: number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'{iteration_name}: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**c**) Implement the Value iteration algorithm. Run the algorithm to obtain the optimal policy for the `FrozenLake-v1` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # Initialize state-value function with zeros for each environment state\n",
    "    V = np.zeros(environment.nS)\n",
    "    for i in range(int(max_iterations)):\n",
    "        # Early stopping condition\n",
    "        delta = 0\n",
    "        # Update each state\n",
    "        for state in range(environment.nS):\n",
    "            # Do a one-step lookahead to calculate state-action values\n",
    "            # action_value =\n",
    "            # Select best action to perform based on the highest state-action value\n",
    "            # best_action_value =\n",
    "            # Calculate change in value\n",
    "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
    "            # Update the value function for current state\n",
    "            V[state] = best_action_value\n",
    "            # Check if we can stop\n",
    "        if delta < theta:\n",
    "            print(f'Value-iteration converged at iteration #{i}.')\n",
    "            break\n",
    "\n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([environment.nS, environment.nA])\n",
    "    for state in range(environment.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        # action_value =\n",
    "        # Select best action based on the highest state-action value\n",
    "        # best_action =\n",
    "        # Update the policy to perform a better action at a current state\n",
    "        policy[state, best_action] = 1.0\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 1000\n",
    "iteration_name = \"Value iteration\"\n",
    "iteration_func = value_iteration\n",
    "# Load a Frozen Lake environment\n",
    "environment = gym.make('FrozenLake-v1')\n",
    "# Search for an optimal policy using policy iteration\n",
    "policy, V = iteration_func(environment.env)\n",
    "# Apply best policy to the real environment\n",
    "wins, total_reward, average_reward = run_episodes(environment, n_episodes, policy)\n",
    "print(f'{iteration_name}: number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'{iteration_name}: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**d**) Compare two optimal policies in part (b) and (c). Which seems to converge faster and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 5000\n",
    "# Functions to find best policy\n",
    "solvers = [('Policy Iteration', policy_iteration),\n",
    "           ('Value Iteration', value_iteration)]\n",
    "for iteration_name, iteration_func in solvers:\n",
    "    # Load a Frozen Lake environment\n",
    "    environment = gym.make('FrozenLake-v1')\n",
    "    # Search for an optimal policy using policy iteration\n",
    "    policy, V = iteration_func(environment.env)\n",
    "    # Apply best policy to the real environment\n",
    "    wins, total_reward, average_reward = run_episodes(environment, n_episodes, policy)\n",
    "    print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "    print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**e**) Run the Policy Iteration and the Value Iteration algorithms on a bigger environment, FrozenLake8x8-v1. Compare the convergence speed of algorithms for both environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "#### **2. Gambler's problem**\n",
    "\n",
    "*(4 x 1.25 points)*\n",
    "\n",
    "Consider an agent placing bets on the outcomes of a sequence of coin flips. It the coin toss results in `heads`, the gambler's return is equal to the bet he placed, if the outcome is `tails`, the opposite happens and the gambler looses the bet. The coin lands on heads 40% of the time. The game continues until the gambler has either lost all his capital or reaches his goal of earning $100$.\n",
    "\n",
    "**Problem summary:**\n",
    "- Undiscounted, finite, episodic MDP: $\\gamma=1$ and there is a terminal state\n",
    "- State: gambler's capital $s \\in \\{ 0, 1, 2, ..., 100 \\}$\n",
    "- Actions: the bet $a = \\{ 0, 1, ..., \\mathrm{min}(s, 100-s)\\}$\n",
    "- Policy: maps the current capital available to how much the agent should bet\n",
    "- Terminal states: capital=0 or capital=100\n",
    "- Reward: 0 for all states, except when the goal is reached (capital=100), where it is +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "goal = 100\n",
    "states = np.arange(goal + 1)\n",
    "\n",
    "p_head = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**a**) The state-value function gives the probability of winning from each state. Perform value iteration and plot the value of each state in different iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initial values\n",
    "state_value = np.zeros(goal + 1)\n",
    "state_value[goal] = 1.0\n",
    "sweeps_history = []\n",
    "\n",
    "# value iteration\n",
    "while True:\n",
    "    old_state_value = state_value.copy()\n",
    "    sweeps_history.append(old_state_value)\n",
    "\n",
    "    for state in states[1:goal]:\n",
    "        # get possilbe actions for current state\n",
    "        # actions =\n",
    "        action_returns = []\n",
    "        for action in actions:\n",
    "            # returns =\n",
    "            action_returns.append(returns)\n",
    "        # new_value =\n",
    "        state_value[state] = new_value\n",
    "    delta = abs(state_value - old_state_value).max()\n",
    "    if delta < 1e-9:\n",
    "        sweeps_history.append(state_value)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for sweep, state_value in enumerate(sweeps_history):\n",
    "    plt.plot(state_value, label='sweep {}'.format(sweep))\n",
    "plt.xlabel('Capital')\n",
    "plt.ylabel('Value estimates')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**b**) Compute and plot the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compute the optimal policy\n",
    "policy = np.zeros(goal + 1)\n",
    "for state in states[1:goal]:\n",
    "    # actions =\n",
    "    action_returns = []\n",
    "    for action in actions:\n",
    "        # returns =\n",
    "        action_returns.append(returns)\n",
    "    policy[state] = actions[np.argmax(np.round(action_returns[1:], 5)) + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(states, policy)\n",
    "plt.xlabel('Capital')\n",
    "plt.ylabel('Final policy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**c**) Re-using your value iteration code provided above, plot the results obtained if $p_{h}=0.25$ and $p_{h}=0.62$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**d**) Compute and plot the corresponding optimal policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}