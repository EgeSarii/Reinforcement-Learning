{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reinforcement Learning 4: *Monte Carlo Methods*\n",
    "\n",
    "**Assignment:** hand-in before 07/03/2022.\n",
    "\n",
    "In this week's assignment, you are going to use Monte Carlo methods to play BlackJack. Remember the task description from the lecture (see also course textbook, Example 5.1).\n",
    "\n",
    "![](https://drive.google.com/uc?id=1R2-H6ra4BqFMJK752OAUNLaKia233V2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's first prepare the visualizations required for the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_blackjack_values(V):\n",
    "    def get_Z(x, y, usable_ace):\n",
    "        if (x,y,usable_ace) in V:\n",
    "            return V[x,y,usable_ace]\n",
    "        else:\n",
    "            return 0\n",
    "    def get_figure(usable_ace, ax):\n",
    "        x_range = np.arange(11, 22)\n",
    "        y_range = np.arange(1, 11)\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "        Z = np.array([get_Z(x,y,usable_ace) for x,y in zip(np.ravel(X), np.ravel(Y))]).reshape(X.shape)\n",
    "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
    "        ax.set_xlabel('Player\\'s Current Sum')\n",
    "        ax.set_ylabel('Dealer\\'s Showing Card')\n",
    "        ax.set_zlabel('State Value')\n",
    "        ax.view_init(ax.elev, -120)\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    ax = fig.add_subplot(211, projection='3d')\n",
    "    ax.set_title('Usable Ace')\n",
    "    get_figure(True, ax)\n",
    "    ax = fig.add_subplot(212, projection='3d')\n",
    "    ax.set_title('No Usable Ace')\n",
    "    get_figure(False, ax)\n",
    "    plt.show()\n",
    "\n",
    "def plot_policy(policy):\n",
    "    def get_Z(x, y, usable_ace):\n",
    "        if (x,y,usable_ace) in policy:\n",
    "            return policy[x,y,usable_ace]\n",
    "        else:\n",
    "            return 1\n",
    "    def get_figure(usable_ace, ax):\n",
    "        x_range = np.arange(11, 22)\n",
    "        y_range = np.arange(10, 0, -1)\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "        Z = np.array([[get_Z(x,y,usable_ace) for x in x_range] for y in y_range])\n",
    "        surf = ax.imshow(Z, cmap=plt.get_cmap('Pastel2', 2), vmin=0, vmax=1, extent=[10.5, 21.5, 0.5, 10.5])\n",
    "        plt.xticks(x_range)\n",
    "        plt.yticks(y_range)\n",
    "        plt.gca().invert_yaxis()\n",
    "        ax.set_xlabel('Player\\'s Current Sum')\n",
    "        ax.set_ylabel('Dealer\\'s Showing Card')\n",
    "        ax.grid(color='w', linestyle='-', linewidth=1)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "        cbar = plt.colorbar(surf, ticks=[0,1], cax=cax)\n",
    "        cbar.ax.set_yticklabels(['0 (STICK)','1 (HIT)'])\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.set_title('Usable Ace')\n",
    "    get_figure(True, ax)\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.set_title('No Usable Ace')\n",
    "    get_figure(False, ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**a**) Load and explore the `Blackjack-v1` environment:\n",
    "\n",
    "*(0.5 Points)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# env ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that each state is a 3-tuple of:\n",
    "- the player's current sum $\\in \\{0, 1, ..., 31\\}$\n",
    "- the dealer's face up card, $\\in \\{1, ..., 10\\}$\n",
    "- whether or not the player has a usable ace (no$=0$, yes$=1$).\n",
    "\n",
    "The agent has two potential actions:\n",
    "- STICK = 0\n",
    "- HIT = 1\n",
    "\n",
    "The return is only computed at the end of an episode:\n",
    "- Loose = -1\n",
    "- Tie = 0\n",
    "- Win = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**b**) Run 10 episodes with a random policy.\n",
    "*(0.5 Points)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def play_episode(env, display=True):\n",
    "    episode = []\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    action_name = [\"STICK\", \"HIT\"]\n",
    "    while True:\n",
    "        # action =\n",
    "        # next_state, reward, done, info =\n",
    "\n",
    "        if display:\n",
    "            print(\"Initial state:\", state)\n",
    "            print(\"Player: \",env.player)\n",
    "            print(\"Dealer: \",env.dealer)\n",
    "            print(action_name[action])\n",
    "            print(\"Current state:\", next_state)\n",
    "\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            rewards.append(reward)\n",
    "            if display:\n",
    "                print(\"Player: \",env.player)\n",
    "                print(\"Dealer: \",env.dealer)\n",
    "                print(\"Reward:\", reward)\n",
    "            break\n",
    "    return episode, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_episodes = 10\n",
    "rw = []\n",
    "for ep in range(n_episodes):\n",
    "    ep, rwe = play_episode(env)\n",
    "    rw.append(rwe[0])\n",
    "print(\"Win: \",np.sum([t for t in rw if t > 0])/n_episodes*100,\"%\")\n",
    "print(\"Tie: \",np.sum([t for t in rw if t == 0])/n_episodes*100,\"%\")\n",
    "print(\"Lost: \",np.sum([t*-1 for t in rw if t < 0])/n_episodes*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**c**) **MC prediction**: Apply the first-visit MC prediction algorithms with Exploring Starts (ES) to estimate the action-value function, $q_{\\pi}(s, a)$ and calculate the value of an initial, random policy.\n",
    "\n",
    "*(3 Points)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_Q(episode, Q, returns_sum, N, gamma=1.0):\n",
    "    \"\"\"\n",
    "    For each time step in the episode we carry out the first visit monte carlo method with exploring starts, checking if this is the first index of this state. Get the discounted reward and add it to the total reward for that state/action pair. Increment the times we have seen this state action pair and finally update the Q values\n",
    "    \"\"\"\n",
    "    for s, a, r in episode:\n",
    "        first_occurence_idx = next(i for i, x in enumerate(episode) if x[0] == s)\n",
    "        G = sum([x[2] * (gamma ** i) for i, x in enumerate(episode[first_occurence_idx:])])\n",
    "        returns_sum[s][a] += G\n",
    "        N[s][a] += 1.0\n",
    "        Q[s][a] = returns_sum[s][a] / N[s][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mc_predict(env, num_episodes, gamma=1.0):\n",
    "    \"\"\"\n",
    "    This is the primary method. Plays through several episodes of the environment.\n",
    "    \"\"\"\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    for i_episode in tqdm(range(1, num_episodes+1)):\n",
    "        # TODO\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# predict the policy values for our random policy, over a large number of episodes\n",
    "sample_episodes = 100000\n",
    "Q = mc_predict(env, sample_episodes)\n",
    "\n",
    "# get the state value function for our test policy\n",
    "V_to_plot = dict((k,(k[0]>18)*(np.dot([0.8, 0.2],v)) + (k[0]<=18)*(np.dot([0.2, 0.8],v)))\n",
    "                 for k, v in Q.items())\n",
    "\n",
    "# plot the state value functions\n",
    "plot_blackjack_values(V_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**d**) **MC control**: Using the methods above to compute the action-value function, $q_{\\pi}(s, a)$, optimize the policy. Use on-policy MC control with $\\epsilon$-greedy action selection.\n",
    "\n",
    "*(5 Points)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_probs(Q_s, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Get the probability of taking the best known action according to epsilon (epsilon-greedy).\n",
    "    Returns the policy for the Q value given\n",
    "    \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon / nA # non-greedy actions\n",
    "    best_a = np.argmax(Q_s)\n",
    "    # policy_s[best_a] = # greedy actions\n",
    "    return policy_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def best_policy(Q, nA):\n",
    "    \"\"\"\n",
    "    returns the best actions for each Q value in the policy\n",
    "    \"\"\"\n",
    "    return dict((k,np.argmax(v)) for k, v in Q.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    \"\"\"\n",
    "    Calculate the new Q values for the actions taken in the given episode.\n",
    "    Returns the new Q value function\n",
    "    \"\"\"\n",
    "    for s, a, r in episode:\n",
    "        first_occurence_idx = next(i for i,x in enumerate(episode) if x[0] == s)\n",
    "        G = sum([x[2]*(gamma**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "        Q[s][a] = Q[s][a] + alpha*(G - Q[s][a])\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def play_game(env, Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    generates an episode from following the epsilon-greedy policy containing the state, action and reward for\n",
    "    each time step in the episode.\n",
    "    Returns all step information for that episode\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        probs = get_probs(Q[state], epsilon, nA)\n",
    "        action = np.random.choice(np.arange(nA), p=probs) if state in Q else env.action_space.sample()\n",
    "        # state, reward, done, info = #\n",
    "        episode.append((state, action, reward))\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes):\n",
    "    \"\"\"\n",
    "    main method. Iterates through episodes updating epsilon after each, retrieves the list of states, actions\n",
    "    and rewards from the last episode and use them to calculate the updated Q values\n",
    "    \"\"\"\n",
    "    epsilon = 1.0\n",
    "    eps_min = 0.01\n",
    "    decay = 0.9999\n",
    "    alpha = 0.001\n",
    "    gamma = 1.0\n",
    "\n",
    "    nA = env.action_space.n\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    for i_episode in tqdm(range(1, num_episodes+1)):\n",
    "        epislon = max(epsilon*decay, eps_min)\n",
    "        # episode =\n",
    "        # Q =\n",
    "    # policy =\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_episodes = 500000\n",
    "policy, Q = mc_control(env, n_episodes)\n",
    "V = dict((k,np.max(v)) for k, v in Q.items())\n",
    "\n",
    "plot_blackjack_values(V)\n",
    "plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**e**) **Sample efficiency**. MC methods require hundreds of thousands of sample episodes to converge. Study how the number of samples affects the learned value functions and policies.\n",
    "\n",
    "*(1 Point)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
